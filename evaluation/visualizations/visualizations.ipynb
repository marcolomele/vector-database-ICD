{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNxn+l1iyWpbkAGwACfNPVZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"NCqg-rdK9b_T","executionInfo":{"status":"ok","timestamp":1748189063577,"user_tz":-120,"elapsed":14975,"user":{"displayName":"Marco Lomele","userId":"15165125036080568143"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.spatial.distance import pdist, squareform\n","from scipy.stats import spearmanr\n","import umap\n","from collections import defaultdict\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maN0Nyks9pwz","executionInfo":{"status":"ok","timestamp":1748189064824,"user_tz":-120,"elapsed":1245,"user":{"displayName":"Marco Lomele","userId":"15165125036080568143"}},"outputId":"59ee5d64-990c-428f-8b8e-1202e11d2d0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def get_chapter_from_code(code):\n","    \"\"\"Extract chapter number from ICD11 code.\"\"\"\n","    code_to_chapter = {'0' : '0',\n","        '1' : '1', '2' : '2', '3' : '3', '4' : '4',\n","        '5' : '5', '6' : '6', '7' : '7', '8' : '8',\n","        '9' : '9', 'A' : '10', 'B' : '11', 'C' : '12',\n","        'D' : '13', 'E' : '14', 'F' : '15', 'G' : '16',\n","        'H' : '17', 'J' : '18', 'K' : '19', 'L' : '20',\n","        'M' : '21', 'N' : '22', 'P' : '23', 'Q' : '24',\n","        'R' : '25', 'S' : '26',\n","        '01' : '1', '02' : '2', '03' : '3', '04' : '4',\n","        '05' : '5', '06' : '6', '07' : '7', '08' : '8',\n","        '09' : '9', '10' : '10', '11' : '11', '12' : '12',\n","        '13' : '13', '14' : '14', '15' : '15', '16' : '16',\n","        '17' : '17', '18' : '18', '19' : '19', '20' : '20',\n","        '21' : '21', '22' : '22', '23' : '23', '24' : '24',\n","        '25' : '25', '26' : '26'\n","    }\n","\n","    if len(code) >= 2 and code[:2].isdigit():\n","        chapter_code = code[:2]\n","    else:\n","        chapter_code = code[:1]\n","\n","    return code_to_chapter.get(chapter_code, None)\n","\n","def load_and_process_embeddings(folder_path=\"Resulting-embeddings\"):\n","    \"\"\"\n","    Function 1: Load CSV files, convert embeddings to numpy arrays, and extract chapter numbers.\n","\n","    Returns:\n","        dict: Dictionary with model names as keys and dictionaries containing 'embeddings', 'codes', and 'chapters' as values\n","    \"\"\"\n","    embeddings_data = {}\n","\n","    # Get all CSV files in the folder\n","    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","\n","    for csv_file in csv_files:\n","        print(f\"Processing {csv_file}...\")\n","\n","        # Extract model name from filename\n","        model_name = csv_file.replace('_ICD11_embeddings.csv', '')\n","\n","        # Read CSV file\n","        file_path = os.path.join(folder_path, csv_file)\n","        df = pd.read_csv(file_path)\n","\n","        # Convert embeddings to numpy arrays\n","        embeddings = []\n","        codes = []\n","        chapters = []\n","\n","        for idx, row in df.iterrows():\n","            try:\n","                # Convert string representation of vector to numpy array\n","                embedding_str = row['Vector'].strip('[]')\n","                embedding_array = np.array([float(x.strip()) for x in embedding_str.split(',')], dtype=np.float64)\n","\n","                # Get chapter from code\n","                chapter = get_chapter_from_code(row['ICD11_code'])\n","\n","                if chapter is not None:  # Only include if we can determine the chapter\n","                    embeddings.append(embedding_array)\n","                    codes.append(row['ICD11_code'])\n","                    chapters.append(chapter)\n","\n","            except Exception as e:\n","                print(f\"Error processing row {idx} in {csv_file}: {e}\")\n","                continue\n","\n","        # Store processed data\n","        embeddings_data[model_name] = {\n","            'embeddings': np.array(embeddings),\n","            'codes': codes,\n","            'chapters': chapters\n","        }\n","\n","        print(f\"Loaded {len(embeddings)} embeddings for {model_name}\")\n","\n","    return embeddings_data\n","\n","def create_tsne_visualization(embeddings_data, save_plots=True):\n","    \"\"\"\n","    Function 2: Create t-SNE visualization of embeddings grouped by chapter.\n","\n","    Args:\n","        embeddings_data: Output from load_and_process_embeddings function\n","        save_plots: Whether to save the plots to files\n","    \"\"\"\n","    colors = ['#FF6F61', '#D2B48C', '#AE65D6', '#598A4F', '#4A90E2', '#F39C12',\n","              '#E74C3C', '#9B59B6', '#1ABC9C', '#34495E', '#F1C40F', '#E67E22',\n","              '#95A5A6', '#3498DB', '#2ECC71', '#E91E63', '#FF9800', '#607D8B',\n","              '#8BC34A', '#FFC107', '#673AB7', '#009688', '#FF5722', '#795548',\n","              '#9E9E9E', '#CDDC39', '#FFEB3B']\n","\n","    for model_name, data in embeddings_data.items():\n","        print(f\"Creating t-SNE visualization for {model_name}...\")\n","\n","        # Perform t-SNE\n","        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n","        embeddings_2d = tsne.fit_transform(data['embeddings'])\n","\n","        # Create plot\n","        plt.figure(figsize=(12, 8))\n","\n","        # Get unique chapters and assign colors\n","        unique_chapters = sorted(list(set(data['chapters'])))\n","        chapter_colors = {chapter: colors[i % len(colors)] for i, chapter in enumerate(unique_chapters)}\n","\n","        # Plot each chapter\n","        for chapter in unique_chapters:\n","            chapter_mask = np.array(data['chapters']) == chapter\n","            chapter_embeddings = embeddings_2d[chapter_mask]\n","\n","            plt.scatter(chapter_embeddings[:, 0], chapter_embeddings[:, 1],\n","                       c=chapter_colors[chapter], label=f'Chapter {chapter}',\n","                       alpha=0.6, s=20)\n","\n","        plt.title(f't-SNE Visualization of {model_name.upper()} Embeddings by ICD11 Chapter', fontsize=14)\n","        plt.xlabel('t-SNE Component 1')\n","        plt.ylabel('t-SNE Component 2')\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","        plt.tight_layout()\n","\n","        if save_plots:\n","            plt.savefig(f'{model_name}_tsne_visualization.png', dpi=300, bbox_inches='tight')\n","            print(f\"Saved t-SNE plot for {model_name}\")\n","\n","        plt.show()\n","\n","def create_umap_visualization(embeddings_data, save_plots=True):\n","    \"\"\"\n","    Function 3: Create UMAP visualization of embeddings grouped by chapter.\n","\n","    Args:\n","        embeddings_data: Output from load_and_process_embeddings function\n","        save_plots: Whether to save the plots to files\n","    \"\"\"\n","    colors = ['#FF6F61', '#D2B48C', '#AE65D6', '#598A4F', '#4A90E2', '#F39C12',\n","              '#E74C3C', '#9B59B6', '#1ABC9C', '#34495E', '#F1C40F', '#E67E22',\n","              '#95A5A6', '#3498DB', '#2ECC71', '#E91E63', '#FF9800', '#607D8B',\n","              '#8BC34A', '#FFC107', '#673AB7', '#009688', '#FF5722', '#795548',\n","              '#9E9E9E', '#CDDC39', '#FFEB3B']\n","\n","    for model_name, data in embeddings_data.items():\n","        print(f\"Creating UMAP visualization for {model_name}...\")\n","\n","        # Perform UMAP\n","        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n","        embeddings_2d = reducer.fit_transform(data['embeddings'])\n","\n","        # Create plot\n","        plt.figure(figsize=(12, 8))\n","\n","        # Get unique chapters and assign colors\n","        unique_chapters = sorted(list(set(data['chapters'])))\n","        chapter_colors = {chapter: colors[i % len(colors)] for i, chapter in enumerate(unique_chapters)}\n","\n","        # Plot each chapter\n","        for chapter in unique_chapters:\n","            chapter_mask = np.array(data['chapters']) == chapter\n","            chapter_embeddings = embeddings_2d[chapter_mask]\n","\n","            plt.scatter(chapter_embeddings[:, 0], chapter_embeddings[:, 1],\n","                       c=chapter_colors[chapter], label=f'Chapter {chapter}',\n","                       alpha=0.6, s=20)\n","\n","        plt.title(f'UMAP Visualization of {model_name.upper()} Embeddings by ICD11 Chapter', fontsize=14)\n","        plt.xlabel('UMAP Component 1')\n","        plt.ylabel('UMAP Component 2')\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","        plt.tight_layout()\n","\n","        if save_plots:\n","            plt.savefig(f'{model_name}_umap_visualization.png', dpi=300, bbox_inches='tight')\n","            print(f\"Saved UMAP plot for {model_name}\")\n","\n","        plt.show()\n","\n","def calculate_trustworthiness_continuity(X_high, X_low, k=10):\n","    \"\"\"\n","    Calculate trustworthiness and continuity metrics for dimensionality reduction quality.\n","\n","    Args:\n","        X_high: High-dimensional embeddings\n","        X_low: Low-dimensional embeddings (t-SNE/UMAP)\n","        k: Number of nearest neighbors to consider\n","\n","    Returns:\n","        tuple: (trustworthiness, continuity)\n","    \"\"\"\n","    n_samples = X_high.shape[0]\n","\n","    # Calculate k-nearest neighbors in both spaces\n","    nbrs_high = NearestNeighbors(n_neighbors=k+1).fit(X_high)\n","    _, indices_high = nbrs_high.kneighbors(X_high)\n","\n","    nbrs_low = NearestNeighbors(n_neighbors=k+1).fit(X_low)\n","    _, indices_low = nbrs_low.kneighbors(X_low)\n","\n","    # Remove self from neighbors\n","    indices_high = indices_high[:, 1:]\n","    indices_low = indices_low[:, 1:]\n","\n","    # Calculate trustworthiness\n","    trustworthiness = 0\n","    for i in range(n_samples):\n","        neighbors_low = set(indices_low[i])\n","        neighbors_high = set(indices_high[i])\n","\n","        # Points that are neighbors in low-dim but not in high-dim\n","        false_neighbors = neighbors_low - neighbors_high\n","\n","        for j in false_neighbors:\n","            # Rank of j in high-dimensional space for point i\n","            rank_high = np.where(indices_high[i] == j)[0]\n","            if len(rank_high) == 0:\n","                # j is not in k-NN of i in high-dim, so rank > k\n","                rank_high = k + np.where(np.argsort(np.linalg.norm(X_high - X_high[i], axis=1))[k+1:] == j)[0]\n","                if len(rank_high) > 0:\n","                    rank_high = k + rank_high[0] + 1\n","                else:\n","                    rank_high = n_samples\n","            else:\n","                rank_high = rank_high[0] + 1\n","\n","            trustworthiness += max(0, rank_high - k)\n","\n","    trustworthiness = 1 - (2 / (n_samples * k * (2 * n_samples - 3 * k - 1))) * trustworthiness\n","\n","    # Calculate continuity\n","    continuity = 0\n","    for i in range(n_samples):\n","        neighbors_high = set(indices_high[i])\n","        neighbors_low = set(indices_low[i])\n","\n","        # Points that are neighbors in high-dim but not in low-dim\n","        missing_neighbors = neighbors_high - neighbors_low\n","\n","        for j in missing_neighbors:\n","            # Rank of j in low-dimensional space for point i\n","            rank_low = np.where(indices_low[i] == j)[0]\n","            if len(rank_low) == 0:\n","                # j is not in k-NN of i in low-dim, so rank > k\n","                rank_low = k + np.where(np.argsort(np.linalg.norm(X_low - X_low[i], axis=1))[k+1:] == j)[0]\n","                if len(rank_low) > 0:\n","                    rank_low = k + rank_low[0] + 1\n","                else:\n","                    rank_low = n_samples\n","            else:\n","                rank_low = rank_low[0] + 1\n","\n","            continuity += max(0, rank_low - k)\n","\n","    continuity = 1 - (2 / (n_samples * k * (2 * n_samples - 3 * k - 1))) * continuity\n","\n","    return trustworthiness, continuity\n","\n","def evaluate_embedding_quality(embeddings_data, save_results=True):\n","    \"\"\"\n","    Function 5: Evaluate embedding quality using multiple clustering metrics.\n","\n","    Args:\n","        embeddings_data: Output from load_and_process_embeddings function\n","        save_results: Whether to save results to CSV files\n","\n","    Returns:\n","        dict: Comprehensive quality metrics for each model\n","    \"\"\"\n","    print(\"Evaluating embedding quality...\")\n","\n","    quality_metrics = {}\n","    all_results = []\n","\n","    for model_name, data in embeddings_data.items():\n","        print(f\"Evaluating {model_name}...\")\n","\n","        embeddings = data['embeddings']\n","        chapters = np.array([int(ch) for ch in data['chapters']])\n","\n","        # Skip if not enough samples or chapters\n","        if len(embeddings) < 10 or len(np.unique(chapters)) < 2:\n","            print(f\"Skipping {model_name}: insufficient data\")\n","            continue\n","\n","        metrics = {}\n","\n","        # 1. Silhouette Score\n","        try:\n","            silhouette = silhouette_score(embeddings, chapters)\n","            metrics['silhouette_score'] = silhouette\n","        except Exception as e:\n","            print(f\"Error calculating silhouette score for {model_name}: {e}\")\n","            metrics['silhouette_score'] = np.nan\n","\n","        # 2. Calinski-Harabasz Index\n","        try:\n","            calinski = calinski_harabasz_score(embeddings, chapters)\n","            metrics['calinski_harabasz_score'] = calinski\n","        except Exception as e:\n","            print(f\"Error calculating Calinski-Harabasz score for {model_name}: {e}\")\n","            metrics['calinski_harabasz_score'] = np.nan\n","\n","        # 3. Davies-Bouldin Index\n","        try:\n","            davies_bouldin = davies_bouldin_score(embeddings, chapters)\n","            metrics['davies_bouldin_score'] = davies_bouldin\n","        except Exception as e:\n","            print(f\"Error calculating Davies-Bouldin score for {model_name}: {e}\")\n","            metrics['davies_bouldin_score'] = np.nan\n","\n","        # 4. Intra-cluster and Inter-cluster distances\n","        try:\n","            unique_chapters = np.unique(chapters)\n","            intra_distances = []\n","            inter_distances = []\n","\n","            for chapter in unique_chapters:\n","                chapter_mask = chapters == chapter\n","                chapter_embeddings = embeddings[chapter_mask]\n","\n","                if len(chapter_embeddings) > 1:\n","                    # Intra-cluster distances\n","                    intra_dist = pdist(chapter_embeddings).mean()\n","                    intra_distances.append(intra_dist)\n","\n","                # Inter-cluster distances\n","                other_embeddings = embeddings[~chapter_mask]\n","                if len(other_embeddings) > 0 and len(chapter_embeddings) > 0:\n","                    inter_dist = np.mean([np.linalg.norm(ce - oe)\n","                                        for ce in chapter_embeddings\n","                                        for oe in other_embeddings[:100]])  # Sample for efficiency\n","                    inter_distances.append(inter_dist)\n","\n","            metrics['mean_intra_cluster_distance'] = np.mean(intra_distances) if intra_distances else np.nan\n","            metrics['mean_inter_cluster_distance'] = np.mean(inter_distances) if inter_distances else np.nan\n","            metrics['cluster_separation_ratio'] = (np.mean(inter_distances) / np.mean(intra_distances)) if intra_distances and inter_distances else np.nan\n","\n","        except Exception as e:\n","            print(f\"Error calculating cluster distances for {model_name}: {e}\")\n","            metrics['mean_intra_cluster_distance'] = np.nan\n","            metrics['mean_inter_cluster_distance'] = np.nan\n","            metrics['cluster_separation_ratio'] = np.nan\n","\n","        # 5. Chapter purity metrics\n","        try:\n","            # Calculate how well chapters are separated\n","            chapter_counts = np.bincount(chapters)\n","            chapter_entropy = -np.sum((chapter_counts / len(chapters)) * np.log2(chapter_counts / len(chapters) + 1e-10))\n","            metrics['chapter_entropy'] = chapter_entropy\n","\n","            # Effective number of chapters (based on entropy)\n","            metrics['effective_num_chapters'] = 2 ** chapter_entropy\n","            metrics['actual_num_chapters'] = len(unique_chapters)\n","\n","        except Exception as e:\n","            print(f\"Error calculating chapter metrics for {model_name}: {e}\")\n","            metrics['chapter_entropy'] = np.nan\n","            metrics['effective_num_chapters'] = np.nan\n","            metrics['actual_num_chapters'] = np.nan\n","\n","        # Store metrics\n","        quality_metrics[model_name] = metrics\n","\n","        # Prepare for DataFrame\n","        result_row = {'model': model_name, **metrics}\n","        all_results.append(result_row)\n","\n","    # Create comprehensive results DataFrame\n","    results_df = pd.DataFrame(all_results)\n","\n","    # Calculate rankings for each metric\n","    ranking_metrics = ['silhouette_score', 'calinski_harabasz_score', 'cluster_separation_ratio']\n","    inverse_ranking_metrics = ['davies_bouldin_score', 'mean_intra_cluster_distance', 'chapter_entropy']\n","\n","    rankings_df = results_df.copy()\n","\n","    for metric in ranking_metrics:\n","        if metric in rankings_df.columns:\n","            rankings_df[f'{metric}_rank'] = rankings_df[metric].rank(ascending=False, na_option='bottom')\n","\n","    for metric in inverse_ranking_metrics:\n","        if metric in rankings_df.columns:\n","            rankings_df[f'{metric}_rank'] = rankings_df[metric].rank(ascending=True, na_option='bottom')\n","\n","    # Calculate overall ranking (average of individual ranks)\n","    rank_columns = [col for col in rankings_df.columns if col.endswith('_rank')]\n","    rankings_df['overall_rank'] = rankings_df[rank_columns].mean(axis=1, skipna=True)\n","    rankings_df['overall_ranking'] = rankings_df['overall_rank'].rank(ascending=True, na_option='bottom')\n","\n","    if save_results:\n","        results_df.to_csv('embedding_quality_metrics.csv', index=False)\n","        rankings_df.to_csv('embedding_quality_rankings.csv', index=False)\n","        print(\"Saved quality metrics and rankings to CSV files\")\n","\n","    return quality_metrics, results_df, rankings_df\n","\n","def evaluate_dimensionality_reduction_quality(embeddings_data, save_results=True):\n","    \"\"\"\n","    Function 6: Evaluate dimensionality reduction quality for t-SNE and UMAP.\n","\n","    Args:\n","        embeddings_data: Output from load_and_process_embeddings function\n","        save_results: Whether to save results to CSV files\n","\n","    Returns:\n","        dict: Dimensionality reduction quality metrics\n","    \"\"\"\n","    print(\"Evaluating dimensionality reduction quality...\")\n","\n","    dr_metrics = {}\n","    all_results = []\n","\n","    for model_name, data in embeddings_data.items():\n","        print(f\"Evaluating dimensionality reduction for {model_name}...\")\n","\n","        embeddings = data['embeddings']\n","        chapters = np.array([int(ch) for ch in data['chapters']])\n","\n","        if len(embeddings) < 10:\n","            print(f\"Skipping {model_name}: insufficient data\")\n","            continue\n","\n","        model_metrics = {}\n","\n","        # Subsample for efficiency if dataset is large\n","        if len(embeddings) > 1000:\n","            indices = np.random.choice(len(embeddings), 1000, replace=False)\n","            embeddings_sample = embeddings[indices]\n","            chapters_sample = chapters[indices]\n","        else:\n","            embeddings_sample = embeddings\n","            chapters_sample = chapters\n","\n","        # t-SNE evaluation\n","        try:\n","            print(f\"  Computing t-SNE for {model_name}...\")\n","            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_sample)//4))\n","            tsne_embeddings = tsne.fit_transform(embeddings_sample)\n","\n","            # Calculate trustworthiness and continuity\n","            trust, cont = calculate_trustworthiness_continuity(embeddings_sample, tsne_embeddings, k=min(10, len(embeddings_sample)//10))\n","\n","            model_metrics['tsne_trustworthiness'] = trust\n","            model_metrics['tsne_continuity'] = cont\n","            model_metrics['tsne_quality_score'] = (trust + cont) / 2\n","\n","            # t-SNE specific: silhouette score in reduced space\n","            tsne_silhouette = silhouette_score(tsne_embeddings, chapters_sample)\n","            model_metrics['tsne_silhouette'] = tsne_silhouette\n","\n","        except Exception as e:\n","            print(f\"Error in t-SNE evaluation for {model_name}: {e}\")\n","            model_metrics['tsne_trustworthiness'] = np.nan\n","            model_metrics['tsne_continuity'] = np.nan\n","            model_metrics['tsne_quality_score'] = np.nan\n","            model_metrics['tsne_silhouette'] = np.nan\n","\n","        # UMAP evaluation\n","        try:\n","            print(f\"  Computing UMAP for {model_name}...\")\n","            reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(embeddings_sample)//5))\n","            umap_embeddings = reducer.fit_transform(embeddings_sample)\n","\n","            # Calculate trustworthiness and continuity\n","            trust, cont = calculate_trustworthiness_continuity(embeddings_sample, umap_embeddings, k=min(10, len(embeddings_sample)//10))\n","\n","            model_metrics['umap_trustworthiness'] = trust\n","            model_metrics['umap_continuity'] = cont\n","            model_metrics['umap_quality_score'] = (trust + cont) / 2\n","\n","            # UMAP specific: silhouette score in reduced space\n","            umap_silhouette = silhouette_score(umap_embeddings, chapters_sample)\n","            model_metrics['umap_silhouette'] = umap_silhouette\n","\n","        except Exception as e:\n","            print(f\"Error in UMAP evaluation for {model_name}: {e}\")\n","            model_metrics['umap_trustworthiness'] = np.nan\n","            model_metrics['umap_continuity'] = np.nan\n","            model_metrics['umap_quality_score'] = np.nan\n","            model_metrics['umap_silhouette'] = np.nan\n","\n","        # Store metrics\n","        dr_metrics[model_name] = model_metrics\n","\n","        # Prepare for DataFrame\n","        result_row = {'model': model_name, **model_metrics}\n","        all_results.append(result_row)\n","\n","    # Create results DataFrame\n","    dr_results_df = pd.DataFrame(all_results)\n","\n","    # Calculate rankings\n","    ranking_metrics = ['tsne_trustworthiness', 'tsne_continuity', 'tsne_quality_score', 'tsne_silhouette',\n","                      'umap_trustworthiness', 'umap_continuity', 'umap_quality_score', 'umap_silhouette']\n","\n","    dr_rankings_df = dr_results_df.copy()\n","\n","    for metric in ranking_metrics:\n","        if metric in dr_rankings_df.columns:\n","            dr_rankings_df[f'{metric}_rank'] = dr_rankings_df[metric].rank(ascending=False, na_option='bottom')\n","\n","    # Calculate overall rankings for t-SNE and UMAP separately\n","    tsne_rank_cols = [col for col in dr_rankings_df.columns if col.startswith('tsne_') and col.endswith('_rank')]\n","    umap_rank_cols = [col for col in dr_rankings_df.columns if col.startswith('umap_') and col.endswith('_rank')]\n","\n","    if tsne_rank_cols:\n","        dr_rankings_df['tsne_overall_rank'] = dr_rankings_df[tsne_rank_cols].mean(axis=1, skipna=True)\n","        dr_rankings_df['tsne_overall_ranking'] = dr_rankings_df['tsne_overall_rank'].rank(ascending=True, na_option='bottom')\n","\n","    if umap_rank_cols:\n","        dr_rankings_df['umap_overall_rank'] = dr_rankings_df[umap_rank_cols].mean(axis=1, skipna=True)\n","        dr_rankings_df['umap_overall_ranking'] = dr_rankings_df['umap_overall_rank'].rank(ascending=True, na_option='bottom')\n","\n","    if save_results:\n","        dr_results_df.to_csv('dimensionality_reduction_metrics.csv', index=False)\n","        dr_rankings_df.to_csv('dimensionality_reduction_rankings.csv', index=False)\n","        print(\"Saved dimensionality reduction metrics and rankings to CSV files\")\n","\n","    return dr_metrics, dr_results_df, dr_rankings_df\n","\n","def create_quality_visualizations(quality_results, dr_results, save_plots=True):\n","    \"\"\"\n","    Create comprehensive visualizations for quality metrics.\n","    \"\"\"\n","    print(\"Creating quality metric visualizations...\")\n","\n","    # 1. Radar chart for embedding quality metrics\n","    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","    # Embedding quality radar chart\n","    ax1 = axes[0, 0]\n","    metrics_to_plot = ['silhouette_score', 'calinski_harabasz_score', 'cluster_separation_ratio']\n","\n","    # Normalize metrics for radar chart\n","    normalized_data = quality_results[1].copy()\n","    for metric in metrics_to_plot:\n","        if metric in normalized_data.columns:\n","            min_val = normalized_data[metric].min()\n","            max_val = normalized_data[metric].max()\n","            if max_val > min_val:\n","                normalized_data[f'{metric}_norm'] = (normalized_data[metric] - min_val) / (max_val - min_val)\n","            else:\n","                normalized_data[f'{metric}_norm'] = 0.5\n","\n","    # Create radar chart\n","    angles = np.linspace(0, 2 * np.pi, len(metrics_to_plot), endpoint=False).tolist()\n","    angles += angles[:1]  # Complete the circle\n","\n","    colors = ['#FF6F61', '#D2B48C', '#AE65D6', '#598A4F', '#4A90E2', '#F39C12', '#E74C3C']\n","\n","    for i, (_, row) in enumerate(normalized_data.iterrows()):\n","        values = [row.get(f'{metric}_norm', 0) for metric in metrics_to_plot]\n","        values += values[:1]  # Complete the circle\n","\n","        ax1.plot(angles, values, 'o-', linewidth=2, label=row['model'], color=colors[i % len(colors)])\n","        ax1.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])\n","\n","    ax1.set_xticks(angles[:-1])\n","    ax1.set_xticklabels([metric.replace('_', ' ').title() for metric in metrics_to_plot])\n","    ax1.set_ylim(0, 1)\n","    ax1.set_title('Embedding Quality Metrics Comparison', fontsize=14)\n","    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","    ax1.grid(True)\n","\n","    # 2. Ranking heatmap\n","    ax2 = axes[0, 1]\n","    ranking_cols = [col for col in quality_results[2].columns if col.endswith('_rank') and not col.startswith('overall')]\n","    if ranking_cols:\n","        ranking_data = quality_results[2][['model'] + ranking_cols].set_index('model')\n","        sns.heatmap(ranking_data.T, annot=True, cmap='RdYlGn_r', ax=ax2, cbar_kws={'label': 'Rank'})\n","        ax2.set_title('Quality Metrics Rankings', fontsize=14)\n","        ax2.set_xlabel('Models')\n","        ax2.set_ylabel('Metrics')\n","\n","    # 3. Dimensionality reduction comparison\n","    ax3 = axes[1, 0]\n","    if len(dr_results[1]) > 0:\n","        dr_comparison = dr_results[1][['model', 'tsne_quality_score', 'umap_quality_score']].set_index('model')\n","        dr_comparison.plot(kind='bar', ax=ax3, color=['#FF6F61', '#AE65D6'])\n","        ax3.set_title('t-SNE vs UMAP Quality Scores', fontsize=14)\n","        ax3.set_ylabel('Quality Score')\n","        ax3.legend(['t-SNE', 'UMAP'])\n","        ax3.tick_params(axis='x', rotation=45)\n","\n","    # 4. Overall rankings summary\n","    ax4 = axes[1, 1]\n","    if 'overall_ranking' in quality_results[2].columns:\n","        overall_rankings = quality_results[2][['model', 'overall_ranking']].sort_values('overall_ranking')\n","        bars = ax4.barh(overall_rankings['model'], overall_rankings['overall_ranking'], color='#598A4F')\n","        ax4.set_title('Overall Quality Rankings', fontsize=14)\n","        ax4.set_xlabel('Ranking (1 = Best)')\n","\n","        # Add value labels on bars\n","        for i, bar in enumerate(bars):\n","            width = bar.get_width()\n","            ax4.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n","                    f'{width:.1f}', ha='left', va='center')\n","\n","    plt.tight_layout()\n","\n","    if save_plots:\n","        plt.savefig('embedding_quality_analysis.png', dpi=300, bbox_inches='tight')\n","        print(\"Saved quality analysis visualization\")\n","\n","    plt.show()\n","\n","def analyze_embeddings(folder_path=\"Resulting-embeddings\", save_plots=True):\n","    \"\"\"\n","    Function 4: Meta function controlling functions 1 to 6.\n","\n","    Args:\n","        folder_path: Path to the folder containing CSV files with embeddings\n","        save_plots: Whether to save visualization plots to files\n","    \"\"\"\n","    print(\"Starting comprehensive embedding analysis...\")\n","    print(\"=\" * 50)\n","\n","    # Function 1: Load and process embeddings\n","    print(\"Step 1: Loading and processing embeddings...\")\n","    embeddings_data = load_and_process_embeddings(folder_path)\n","\n","    if not embeddings_data:\n","        print(\"No embeddings data loaded. Please check the folder path and CSV files.\")\n","        return\n","\n","    print(f\"Successfully loaded embeddings for {len(embeddings_data)} models\")\n","    print(\"=\" * 50)\n","\n","    # Function 5: Evaluate embedding quality\n","    print(\"Step 2: Evaluating embedding quality...\")\n","    quality_metrics, quality_results_df, quality_rankings_df = evaluate_embedding_quality(embeddings_data, save_plots)\n","    print(\"=\" * 50)\n","\n","    # Function 6: Evaluate dimensionality reduction quality\n","    print(\"Step 3: Evaluating dimensionality reduction quality...\")\n","    dr_metrics, dr_results_df, dr_rankings_df = evaluate_dimensionality_reduction_quality(embeddings_data, save_plots)\n","    print(\"=\" * 50)\n","\n","    # Create quality visualizations\n","    print(\"Step 4: Creating quality visualizations...\")\n","    create_quality_visualizations((quality_metrics, quality_results_df, quality_rankings_df),\n","                                (dr_metrics, dr_results_df, dr_rankings_df), save_plots)\n","    print(\"=\" * 50)\n","\n","    # Function 2: Create t-SNE visualizations\n","    print(\"Step 5: Creating t-SNE visualizations...\")\n","    create_tsne_visualization(embeddings_data, save_plots)\n","    print(\"=\" * 50)\n","\n","    # Function 3: Create UMAP visualizations\n","    print(\"Step 6: Creating UMAP visualizations...\")\n","    create_umap_visualization(embeddings_data, save_plots)\n","    print(\"=\" * 50)\n","\n","    print(\"Analysis complete!\")\n","\n","    # Print comprehensive summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"COMPREHENSIVE ANALYSIS SUMMARY FOR REPORT APPENDIX\")\n","    print(\"=\"*80)\n","\n","    # Quality metrics summary\n","    print(\"\\n1. EMBEDDING QUALITY METRICS:\")\n","    print(\"-\" * 40)\n","    if not quality_results_df.empty:\n","        print(quality_results_df.round(4).to_string(index=False))\n","\n","        print(\"\\n2. QUALITY RANKINGS:\")\n","        print(\"-\" * 40)\n","        ranking_summary = quality_rankings_df[['model', 'silhouette_score_rank', 'calinski_harabasz_score_rank',\n","                                             'davies_bouldin_score_rank', 'overall_ranking']].round(2)\n","        print(ranking_summary.to_string(index=False))\n","\n","    # Dimensionality reduction summary\n","    print(\"\\n3. DIMENSIONALITY REDUCTION QUALITY:\")\n","    print(\"-\" * 40)\n","    if not dr_results_df.empty:\n","        print(dr_results_df.round(4).to_string(index=False))\n","\n","        print(\"\\n4. DIMENSIONALITY REDUCTION RANKINGS:\")\n","        print(\"-\" * 40)\n","        dr_ranking_summary = dr_rankings_df[['model', 'tsne_overall_ranking', 'umap_overall_ranking']].round(2)\n","        print(dr_ranking_summary.to_string(index=False))\n","\n","    # Best performing models summary\n","    print(\"\\n5. TOP PERFORMING MODELS:\")\n","    print(\"-\" * 40)\n","    if not quality_rankings_df.empty:\n","        best_overall = quality_rankings_df.loc[quality_rankings_df['overall_ranking'].idxmin(), 'model']\n","        print(f\"Best Overall Quality: {best_overall}\")\n","\n","        best_silhouette = quality_results_df.loc[quality_results_df['silhouette_score'].idxmax(), 'model']\n","        print(f\"Best Silhouette Score: {best_silhouette}\")\n","\n","        if not dr_rankings_df.empty:\n","            best_tsne = dr_rankings_df.loc[dr_rankings_df['tsne_overall_ranking'].idxmin(), 'model']\n","            best_umap = dr_rankings_df.loc[dr_rankings_df['umap_overall_ranking'].idxmin(), 'model']\n","            print(f\"Best t-SNE Quality: {best_tsne}\")\n","            print(f\"Best UMAP Quality: {best_umap}\")\n","\n","    print(\"\\n6. FILES GENERATED FOR APPENDIX:\")\n","    print(\"-\" * 40)\n","    print(\"- embedding_quality_metrics.csv\")\n","    print(\"- embedding_quality_rankings.csv\")\n","    print(\"- dimensionality_reduction_metrics.csv\")\n","    print(\"- dimensionality_reduction_rankings.csv\")\n","    print(\"- embedding_quality_analysis.png\")\n","    print(\"- Individual t-SNE and UMAP visualization PNGs\")\n","\n","    return {\n","        'embeddings_data': embeddings_data,\n","        'quality_metrics': quality_metrics,\n","        'quality_results': quality_results_df,\n","        'quality_rankings': quality_rankings_df,\n","        'dr_metrics': dr_metrics,\n","        'dr_results': dr_results_df,\n","        'dr_rankings': dr_rankings_df\n","    }"],"metadata":{"id":"bZPZGfuhKdLK","executionInfo":{"status":"ok","timestamp":1748189254425,"user_tz":-120,"elapsed":18,"user":{"displayName":"Marco Lomele","userId":"15165125036080568143"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["results = analyze_embeddings(folder_path='/content/drive/MyDrive/Education/University/Master/NLP-Bros/Embeddings-final/Resulting-embeddings')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HB-6LC8Gjbe8m6-pbhq_QW1phWmxaaVh"},"id":"gkW1evyQKkdZ","executionInfo":{"status":"ok","timestamp":1748190246955,"user_tz":-120,"elapsed":979832,"user":{"displayName":"Marco Lomele","userId":"15165125036080568143"}},"outputId":"2f330d78-8137-4515-a30e-35782407bf55"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}