{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk_setup import setup_nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK punkt is already downloaded\n",
      "NLTK averaged_perceptron_tagger is already downloaded\n",
      "NLTK stopwords is already downloaded\n"
     ]
    }
   ],
   "source": [
    "setup_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'code', 'title', 'browser_url', 'class_kind', 'definition',\n",
       "       'parent', 'inclusions', 'foundation_children',\n",
       "       'foundation_child_references', 'index_terms', 'related_entities',\n",
       "       'full_text', 'children', 'postcoordination_scales',\n",
       "       'index_term_references', 'exclusions', 'exclusion_references',\n",
       "       'fully_specified_name', 'generated_description', 'chapter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = pd.read_csv('icd11-25_data_clean_with_generated_descriptions.csv')\n",
    "dg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13960 entries, 0 to 13959\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   id                           13960 non-null  int64 \n",
      " 1   code                         13062 non-null  object\n",
      " 2   title                        13960 non-null  object\n",
      " 3   browser_url                  13960 non-null  object\n",
      " 4   class_kind                   13960 non-null  object\n",
      " 5   definition                   6894 non-null   object\n",
      " 6   parent                       13960 non-null  int64 \n",
      " 7   inclusions                   1113 non-null   object\n",
      " 8   foundation_children          1468 non-null   object\n",
      " 9   foundation_child_references  1468 non-null   object\n",
      " 10  index_terms                  10855 non-null  object\n",
      " 11  related_entities             2287 non-null   object\n",
      " 12  full_text                    13960 non-null  object\n",
      " 13  children                     3282 non-null   object\n",
      " 14  postcoordination_scales      7235 non-null   object\n",
      " 15  index_term_references        3806 non-null   object\n",
      " 16  exclusions                   2404 non-null   object\n",
      " 17  exclusion_references         2380 non-null   object\n",
      " 18  fully_specified_name         89 non-null     object\n",
      " 19  generated_description        13960 non-null  object\n",
      " 20  chapter                      13960 non-null  int64 \n",
      "dtypes: int64(3), object(18)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, \n",
    "                   to_lowercase=True, \n",
    "                   remove_punctuation=True, \n",
    "                   lemmatize=True,\n",
    "                   remove_stopwords=True):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if to_lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def create_preprocessed_columns(df, original_col='definition',\n",
    "                                generated_col='generated_description',\n",
    "                                to_lowercase=True,\n",
    "                                remove_punctuation=True,\n",
    "                                lemmatize=True,\n",
    "                                remove_stopwords=True):\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    df_processed = df[['id', original_col, generated_col]].copy()\n",
    "    \n",
    "    # Create new column names for preprocessed text\n",
    "    original_processed = f\"{original_col}_processed\"\n",
    "    generated_processed = f\"{generated_col}_processed\"\n",
    "    \n",
    "    # Apply preprocessing to both columns\n",
    "    df_processed[original_processed] = df_processed[original_col].apply(\n",
    "        lambda x: preprocess_text(x, \n",
    "                                to_lowercase=to_lowercase,\n",
    "                                remove_punctuation=remove_punctuation,\n",
    "                                lemmatize=lemmatize,\n",
    "                                remove_stopwords=remove_stopwords)\n",
    "    )\n",
    "    \n",
    "    df_processed[generated_processed] = df_processed[generated_col].apply(\n",
    "        lambda x: preprocess_text(x,\n",
    "                                to_lowercase=to_lowercase,\n",
    "                                remove_punctuation=remove_punctuation,\n",
    "                                lemmatize=lemmatize,\n",
    "                                remove_stopwords=remove_stopwords)\n",
    "    )\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_df = create_preprocessed_columns(\n",
    "    dg,\n",
    "    to_lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    lemmatize=False,\n",
    "    remove_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabolary Richness / Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recongition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13960 entries, 0 to 13959\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   id                           13960 non-null  int64 \n",
      " 1   code                         13062 non-null  object\n",
      " 2   title                        13960 non-null  object\n",
      " 3   browser_url                  13960 non-null  object\n",
      " 4   class_kind                   13960 non-null  object\n",
      " 5   definition                   6894 non-null   object\n",
      " 6   parent                       13960 non-null  int64 \n",
      " 7   inclusions                   1113 non-null   object\n",
      " 8   foundation_children          1468 non-null   object\n",
      " 9   foundation_child_references  1468 non-null   object\n",
      " 10  index_terms                  10855 non-null  object\n",
      " 11  related_entities             2287 non-null   object\n",
      " 12  full_text                    13960 non-null  object\n",
      " 13  children                     3282 non-null   object\n",
      " 14  postcoordination_scales      7235 non-null   object\n",
      " 15  index_term_references        3806 non-null   object\n",
      " 16  exclusions                   2404 non-null   object\n",
      " 17  exclusion_references         2380 non-null   object\n",
      " 18  fully_specified_name         89 non-null     object\n",
      " 19  generated_description        13960 non-null  object\n",
      " 20  chapter                      13960 non-null  int64 \n",
      "dtypes: int64(3), object(18)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = dg[['id', 'chapter', 'code', 'title', 'generated_description', 'inclusions', 'related_entities', 'children', 'exclusions']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorization_text(df):\n",
    "    # Create a copy of the dataframe\n",
    "    df_vector = df.copy()\n",
    "    \n",
    "    # Create a dictionary for quick title lookup\n",
    "    title_lookup = dict(zip(df['id'], df['title']))\n",
    "    \n",
    "    def process_row(row):\n",
    "        # Start with the generated description\n",
    "        vector_text = row['generated_description']\n",
    "        \n",
    "        # Get the condition name from title\n",
    "        condition_name = row['title']\n",
    "        \n",
    "        # Process inclusions if available\n",
    "        if pd.notna(row['inclusions']):\n",
    "            # Split inclusions into list and clean\n",
    "            inclusion_terms = [term.strip() for term in row['inclusions'].split(';')]\n",
    "            # Filter out terms already in the description\n",
    "            new_terms = [term for term in inclusion_terms \n",
    "                        if term.lower() not in vector_text.lower()]\n",
    "            if new_terms:\n",
    "                vector_text += f\"\\n{condition_name} includes the following diagnostic terms and synonyms: {', '.join(new_terms)}.\"\n",
    "        \n",
    "            # Process related entities if available\n",
    "        if pd.notna(row['related_entities']):\n",
    "            # Split related entities into list and clean\n",
    "            related_ids = [id.strip() for id in row['related_entities'].split(';')]\n",
    "            # Get titles for each ID\n",
    "            related_titles = [title_lookup.get(int(id), '') for id in related_ids if id]\n",
    "            related_titles = [title for title in related_titles if title]  # Remove empty strings\n",
    "            # Filter out titles that are already mentioned in the text\n",
    "            new_related_titles = [title for title in related_titles \n",
    "                                if title.lower() not in vector_text.lower()]\n",
    "            if new_related_titles:\n",
    "                vector_text += f\"\\n{condition_name} is clinically related to: {', '.join(new_related_titles)}.\"\n",
    "        \n",
    "        # Process children if available\n",
    "        if pd.notna(row['children']):\n",
    "            # Split children into list and clean\n",
    "            child_ids = [id.strip() for id in row['children'].split(';')]\n",
    "            # Get titles for each ID\n",
    "            child_titles = [title_lookup.get(int(id), '') for id in child_ids if id and id not in ('other', 'unspecified')]\n",
    "            child_titles = [title for title in child_titles if title]  # Remove empty strings\n",
    "            if child_titles:\n",
    "                vector_text += f\"\\n{condition_name} is a parent category that includes the following specific conditions: {', '.join(child_titles)}.\"\n",
    "        \n",
    "        # Process exclusions if available\n",
    "        if pd.notna(row['exclusions']):\n",
    "            # Split exclusions into list and clean\n",
    "            exclusion_terms = [term.strip() for term in row['exclusions'].split(';')]\n",
    "            # Filter out terms already in the description\n",
    "            new_terms = [term for term in exclusion_terms \n",
    "                        if term.lower() not in vector_text.lower()]\n",
    "            if new_terms:\n",
    "                vector_text += f\"\\nTerms that are not categorized under {condition_name} are: {', '.join(new_terms)}.\"\n",
    "        \n",
    "        return vector_text\n",
    "    \n",
    "    # Apply the processing function to each row\n",
    "    df_vector['vectorization_text'] = df_vector.apply(process_row, axis=1)\n",
    "    \n",
    "    return df_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_vectorization = create_vectorization_text(dv)[['id', 'chapter', 'code', 'title', 'vectorization_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_vectorization.to_csv('icd11-25_data_vectorization.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_icd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
